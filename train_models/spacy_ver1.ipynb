{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MR1Wu6xk9IS9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import spacy "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import training dataset "
      ],
      "metadata": {
        "id": "-opW5jdjEngs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00nr3H-Q9Rzb",
        "outputId": "a6919b41-8b14-4ecc-8872-f242cc03c05c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Maßnahmenbekanntgabe zu MA 40, Prüfung der Nebenbeschäftigungen',\n",
              " {'entities': [(24, 29, 'ORG')]})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def get_data(path):\n",
        "    with open(path,'r', encoding = 'utf-8') as f:\n",
        "        data = [json.loads(line) for line in f]\n",
        "    return data\n",
        "\n",
        "def to_train(data):\n",
        "    # train data : (\"Oranges are great source of vitamin C\",{\"entities\":[(0,7,\"Fruit\")]}\n",
        "    \n",
        "    train = []\n",
        "\n",
        "    for i in data:\n",
        "        txt = i['text']\n",
        "        ent = []\n",
        "\n",
        "        for j in i['entities']:\n",
        "            s = j['start_offset']\n",
        "            e = j['end_offset']\n",
        "            l = j['label']\n",
        "            ent.append((s,e,l))\n",
        "        train.append((txt,{\"entities\":ent}))\n",
        "\n",
        "    return train\n",
        "\n",
        "\n",
        "path = 'all.jsonl'\n",
        "data_all = to_train(get_data(path))\n",
        "data_all[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data_all, test_size = 0.3)"
      ],
      "metadata": {
        "id": "Fj2ISu8a-ytG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change to .spacy file"
      ],
      "metadata": {
        "id": "_q3SwSocEuue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.util import filter_spans\n",
        "from tqdm import tqdm\n",
        "from spacy.tokens import DocBin\n",
        "nlp = spacy.blank(\"de\") # load a new spacy model\n",
        "# doc_bin = DocBin() # create a DocBin object\n",
        "\n",
        "def to_spacy(data,output_savepath):\n",
        " \n",
        "  # nlp = spacy.blank(\"de\") # load a new spacy model\n",
        "  doc_bin = DocBin() # create a DocBin object\n",
        "  \n",
        "  for training_example in tqdm(data): \n",
        "\n",
        "      text = training_example[0] #extract sentence\n",
        "      labels = training_example[1]['entities']\n",
        "      \n",
        "      doc = nlp.make_doc(text) \n",
        "      ents = []\n",
        "\n",
        "      for start, end, label in labels:\n",
        "          span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "          if span is None:\n",
        "              print(\"Skipping entity\")\n",
        "          else:\n",
        "              ents.append(span)\n",
        "\n",
        "      filtered_ents = filter_spans(ents)\n",
        "      doc.ents = filtered_ents \n",
        "      doc_bin.add(doc)\n",
        "\n",
        "  doc_bin.to_disk(output_savepath) # save the docbin object"
      ],
      "metadata": {
        "id": "7CELMDVX-cmV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_spacy(train,\"./train.spacy\")\n",
        "to_spacy(test,\"./test.spacy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fULQ3BakAmFV",
        "outputId": "f143690d-8377-4bfe-a8d6-81bd752a9b7b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▋     | 1205/2593 [00:00<00:00, 2112.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping entity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2593/2593 [00:01<00:00, 2117.33it/s]\n",
            "100%|██████████| 1112/1112 [00:00<00:00, 1646.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "RedBJVUBE1U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create config file: https://spacy.io/usage/training#config\n",
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqSlKGkhAxHm",
        "outputId": "888f8fb0-13ee-4221-abaf-48a401deef56"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-10-09 13:51:32.790555: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./test.spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpcODzV6BAl8",
        "outputId": "a68a6350-d336-40e1-db28-b888db36689a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-10-09 13:51:56.301458: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-10-09 13:51:57,006] [INFO] Set up nlp object from config\n",
            "INFO:spacy:Set up nlp object from config\n",
            "[2022-10-09 13:51:57,018] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "INFO:spacy:Pipeline: ['tok2vec', 'ner']\n",
            "[2022-10-09 13:51:57,023] [INFO] Created vocabulary\n",
            "INFO:spacy:Created vocabulary\n",
            "[2022-10-09 13:51:57,024] [INFO] Finished initializing nlp object\n",
            "INFO:spacy:Finished initializing nlp object\n",
            "[2022-10-09 13:52:00,604] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "INFO:spacy:Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     16.83    0.00    0.00    0.00    0.00\n",
            "  0     200         41.23    882.79   83.95   84.26   83.64    0.84\n",
            "  0     400         67.85    177.19   92.24   93.37   91.13    0.92\n",
            "  0     600         91.21    146.00   93.49   91.50   95.56    0.93\n",
            "  0     800        330.45    222.95   88.76   91.14   86.51    0.89\n",
            "  1    1000        204.47    257.25   94.31   93.62   95.01    0.94\n",
            "  1    1200        287.70    242.60   93.44   92.80   94.09    0.93\n",
            "  2    1400        197.27    221.01   95.06   93.32   96.86    0.95\n",
            "  2    1600        298.18    280.90   94.46   92.86   96.12    0.94\n",
            "  3    1800        279.69    255.22   95.75   94.67   96.86    0.96\n",
            "  4    2000        401.01    315.70   95.44   94.23   96.67    0.95\n",
            "  5    2200       1093.15    280.15   95.72   94.27   97.23    0.96\n",
            "  7    2400        536.08    293.60   94.71   92.66   96.86    0.95\n",
            "  8    2600        570.67    232.35   95.05   94.19   95.93    0.95\n",
            " 10    2800        469.85    196.81   94.61   93.50   95.75    0.95\n",
            " 11    3000        513.39    190.81   95.52   94.57   96.49    0.96\n",
            " 13    3200        893.84    157.76   94.68   92.36   97.13    0.95\n",
            " 14    3400        354.57    140.34   95.37   93.67   97.13    0.95\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test trained model"
      ],
      "metadata": {
        "id": "TIkbZydGE74M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_nlp = spacy.load(\"output/model-best\")\n",
        "text = \"Organigramm der Wiener Rettungsleitstelle Quelle: Magistratsabteilung 70, Darstellung: Stadtrechnungshof Wien.\"\n",
        "doc = trained_nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJFm1u5bEMDM",
        "outputId": "34583c04-7763-4f10-9104-15af4efc8f75"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Magistratsabteilung 70 ORG\n",
            "Stadtrechnungshof Wien ORG\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
