{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70ff9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sooje\\anaconda3\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "# !python -m spacy download de_core_news_sm\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc86c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    print(\"Reading file...\")\n",
    "    with open(path,'r', encoding = 'utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data\n",
    "\n",
    "def iob(data):\n",
    "    count = 0 \n",
    "    i=0\n",
    "    starts = [x['start_offset'] for x in data['entities']]\n",
    "    ends = [x['end_offset'] for x in data['entities']]\n",
    "    starts.append(99999)\n",
    "    ends.append(99999)\n",
    "\n",
    "    sen_temp = []\n",
    "    tag_temp = []\n",
    "\n",
    "    text = data['text']\n",
    "    if text.startswith(\" \"):\n",
    "        text = text[1:]\n",
    "        count = 1\n",
    "    doc = nlp(text)\n",
    "\n",
    "\n",
    "    for t in doc:\n",
    "        sen_temp.append(t.text)\n",
    "        if count == starts[i]:\n",
    "            tag_temp.append('B-ORG')\n",
    "            count = count+len(t.text)+1\n",
    "            if count > ends[i]:\n",
    "                i += 1\n",
    "\n",
    "        elif count > starts[i] and count < ends[i]:\n",
    "            if tag_temp[-1] == 'O':\n",
    "                tag_temp.append('B-ORG')\n",
    "            else:\n",
    "                tag_temp.append('I-ORG')\n",
    "            count = count+len(t.text)+1\n",
    "            if count > ends[i]:\n",
    "                i += 1\n",
    "\n",
    "        else:\n",
    "            tag_temp.append('O')\n",
    "            count = count+len(t.text)+1\n",
    "\n",
    "    return sen_temp, tag_temp\n",
    "\n",
    "def tokenized_output(data):\n",
    "    print('Tokenizing...')\n",
    "    doc_id_list = []\n",
    "    para_id_list = []\n",
    "    txt_list = []\n",
    "    tag_list = []\n",
    "\n",
    "    for d in data:\n",
    "        doc_id_list.append(d['doc_id'])\n",
    "        para_id_list.append(d['para_id'])\n",
    "        txt, tag = iob(d)\n",
    "        txt_list.append(txt)\n",
    "        tag_list.append(tag)\n",
    "\n",
    "    tokenized = pd.DataFrame({'doc_id' : doc_id_list,\n",
    "                             'para_id': para_id_list,\n",
    "                             'tokens' : txt_list,\n",
    "                             'ner_tags' : tag_list})\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6078539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iob_with_pos(d):\n",
    "    count = 0 \n",
    "    i=0\n",
    "    starts = [x['start_offset'] for x in d['entities']]\n",
    "    ends = [x['end_offset'] for x in d['entities']]\n",
    "    starts.append(99999)\n",
    "    ends.append(99999)\n",
    "\n",
    "    sen_temp = []\n",
    "    tag_temp = []\n",
    "\n",
    "    \n",
    "    text = d['text']\n",
    "    if text.startswith(\" \"):\n",
    "        text = text[1:]\n",
    "        count = 1\n",
    "    doc = nlp(text)\n",
    "    pos = [token.tag_ for token in doc]\n",
    "\n",
    "    for t in doc:\n",
    "        sen_temp.append(t.text)\n",
    "        if count == starts[i]:\n",
    "            tag_temp.append('B-ORG')\n",
    "            count = count+len(t.text)+1\n",
    "            if count > ends[i]:\n",
    "                i += 1\n",
    "\n",
    "        elif count > starts[i] and count < ends[i]:\n",
    "            if tag_temp[-1] == 'O':\n",
    "                tag_temp.append('B-ORG')\n",
    "            else:\n",
    "                tag_temp.append('I-ORG')\n",
    "            count = count+len(t.text)+1\n",
    "            if count > ends[i]:\n",
    "                i += 1\n",
    "\n",
    "        else:\n",
    "            tag_temp.append('O')\n",
    "            count = count+len(t.text)+1\n",
    "    token_nums = len(tag_temp)\n",
    "    doc_id_temp = [str(d['doc_id'])+str(d['para_id'])] * token_nums\n",
    "    return sen_temp, pos, tag_temp\n",
    "\n",
    "\n",
    "def to_conll_txt(tokenized,filename):\n",
    "    with open(\"{}.txt\".formant(filename), \"w\", encoding = 'utf-8') as record_file:\n",
    "        for i in tokenized:\n",
    "            sen, pos, tag = iob_with_pos(i)\n",
    "            for j in range(len(sen)):\n",
    "                record_file.write(sen[j]+\"\\t\"+pos[j]+\"\\t\"+tag[j]+\"\\n\")\n",
    "            record_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2ad0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file...\n",
      "Tokenizing...\n",
      "   doc_id  para_id                                             tokens  \\\n",
      "0       0        0  [Maßnahmenbekanntgabe, zu, MA, 40, ,, Prüfung,...   \n",
      "1       0        1                               [INHALTSVERZEICHNIS]   \n",
      "2       0        2                            [ABKÜRZUNGSVERZEICHNIS]   \n",
      "3       0        3               [bzw., beziehungsweise, Nr., Nummer]   \n",
      "4       0        4  [Erledigung, des, Prüfungsberichtes, Der, Stad...   \n",
      "\n",
      "                                            ner_tags  \n",
      "0                   [O, O, B-ORG, I-ORG, O, O, O, O]  \n",
      "1                                                [O]  \n",
      "2                                                [O]  \n",
      "3                                       [O, O, O, O]  \n",
      "4  [O, O, O, O, B-ORG, I-ORG, O, O, O, O, O, O, O...  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    path = 'all.jsonl'\n",
    "    data = get_data(path)\n",
    "    tokenized = tokenized_output(data)\n",
    "    print(tokenized.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4d87321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die O\n",
      "Magistratsabteilung B-ORG\n",
      "40 I-ORG\n",
      "sowie O\n",
      "der O\n",
      "Fonds B-ORG\n",
      "Soziales I-ORG\n",
      "Wien I-ORG\n",
      "und O\n",
      "das O\n",
      "Kuratorium B-ORG\n",
      "Wiener I-ORG\n",
      "Pensionisten-Wohnhäuser I-ORG\n",
      "( O\n",
      "in O\n",
      "Summe O\n",
      "somit O\n",
      "zwei O\n",
      "Drittel O\n",
      "aller O\n",
      "geprüften O\n",
      "Einrichtungen O\n",
      ") O\n",
      "verfügten O\n",
      "über O\n",
      "eine O\n",
      "Stelle O\n",
      "für O\n",
      "Interne O\n",
      "Revision O\n",
      ", O\n",
      "während O\n",
      "das O\n",
      "übrige O\n",
      "Drittel O\n",
      "eine O\n",
      "solche O\n",
      "nicht O\n",
      "eingerichtet O\n",
      "hatte O\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "# def print_result(i):\n",
    "#     for j in range(len(tokenized.iloc[i].tokens)):\n",
    "#         print(tokenized.iloc[i].tokens[j], tokenized.iloc[i].ner_tags[j])\n",
    "# print_result(300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
